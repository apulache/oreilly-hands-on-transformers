{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32c924c",
   "metadata": {},
   "source": [
    "## Off the shelf results with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e56f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for gpu to use. Will use 'cpu' by default if no gpu found.\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"../\"\n",
    "remote_path = \"/notebooks/oreilly-hands-on-transformers/\"\n",
    "\n",
    "#runtime_path = remote_path\n",
    "runtime_path = local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883b7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c21f6b",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd740019",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize =\"\"\"Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay Area with his dog, \n",
    "Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics \n",
    "at Johns Hopkins University before transitioning to education. He spent several years conducting lectures \n",
    "on data science at Johns Hopkins University and at the General Assembly before founding his own startup, \n",
    "Kylie.ai, which uses artificial intelligence to build chatbots from historical transcripts. \n",
    "After completing a Fellowship at the Y Combinator accelerator, Sinan spent most of his time working on \n",
    "his fast-growing company, while creating educational material for data science.\n",
    "\"\"\"\n",
    "\n",
    "preprocess_text = text_to_summarize.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "print (\"original text preprocessed:\\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known prompt for summarization with T5\n",
    "t5_prepared_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "input_ids = base_tokenizer.encode(t5_prepared_text, return_tensors=\"pt\")\n",
    "\n",
    "# summmarize \n",
    "summary_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    min_length=30,\n",
    "    max_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Summarized text: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65fa05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ac67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cd997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba2c628",
   "metadata": {},
   "source": [
    "## English -> German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f956b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode('translate English to German: Where is the chocolate?', return_tensors='pt')\n",
    "\n",
    "# translate \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Translated text:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb64ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass labels in to calculate loss\n",
    "\n",
    "input_ids = base_tokenizer('translate English to German: Where is the chocolate?', return_tensors='pt').input_ids\n",
    "labels = base_tokenizer('Wo ist die Schokolade?', return_tensors='pt').input_ids\n",
    "\n",
    "loss = base_model(input_ids=input_ids, labels=labels).loss\n",
    "\n",
    "labels, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3012d5e3",
   "metadata": {},
   "source": [
    "## CoLA: The Corpus of Linguistic Acceptability\n",
    "checking for grammatical correctess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada39336",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where is the chocolate?', return_tensors='pt')\n",
    "\n",
    "# CoLA \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where be a chocolates?', return_tensors='pt')\n",
    "\n",
    "# summmarize \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb731953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "509b5967",
   "metadata": {},
   "source": [
    "## STSB - Semantic Text Similarity Benchmark\n",
    "Are two sentences semantically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Fishing Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Hiking Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea1065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3081d51",
   "metadata": {},
   "source": [
    "## MNLI - Multi-Genre Natural Language Inference\n",
    "Whether a premise implies (“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3808dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I am running for mayor', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I do not really vote', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I code for a living', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aecb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca923c09",
   "metadata": {},
   "source": [
    "## Q/A - Question/Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Sinan live? context: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4845657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Matt live? context: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d4fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check with random prompts\n",
    "\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'prompt1: Where does Matt live? prompt2: Sinan lives in California but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913ab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21434974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "336c91b9",
   "metadata": {},
   "source": [
    "## Fine-tuning T5 for abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5ForConditionalGeneration, TrainingArguments, Trainer, \\\n",
    "                         DataCollatorForSeq2Seq\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/snap/amazon-fine-food-reviews?select=Reviews.csv\n",
    "\n",
    "reviews = pd.read_csv(runtime_path + 'data/reviews.csv')\n",
    "\n",
    "# Pre-processing step\n",
    "# Punctuation is important in grammar and important for complex decoding architectures to know when to stop!\n",
    "def add_punc(s):\n",
    "    if s[-1] not in ('.', '!', '?'):\n",
    "        s = s + '.'\n",
    "    return s\n",
    "\n",
    "reviews.dropna(inplace=True)\n",
    "\n",
    "reviews['Summary'] = reviews['Summary'].map(add_punc)\n",
    "\n",
    "print(reviews.shape)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[(reviews['Summary'].str.len() < 100) & (reviews['Summary'].str.len() >= 30)]\n",
    "\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "reviews_dataset = Dataset.from_pandas(reviews.astype(str).sample(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a prompt but only as a prefix in the encoder\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "# we will manually add our own labels because unlike GPT, we cannot assume the labels are based on the inputs\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = base_tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = base_tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822208b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews_dataset = reviews_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a single datapoint\n",
    "tokenized_reviews_dataset['Summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews_dataset = tokenized_reviews_dataset.train_test_split(test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5565f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator specifically for generic sequence to sequence tasks\n",
    "# Use when we are translating one sequence to another like translation, summarization, etc\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=base_tokenizer, model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24d2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=runtime_path + \"t5_summary_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_reviews_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_reviews_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1671d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54510733",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1d16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = T5ForConditionalGeneration.from_pretrained(runtime_path + 't5_summary_results')\n",
    "\n",
    "# summarization pipeline prepends a default prefix of summarize: \n",
    "generator = pipeline(\n",
    "    'summarization', model=loaded_model, tokenizer=base_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = reviews.sample(1)\n",
    "\n",
    "print(sam['Summary'])\n",
    "\n",
    "text = sam['Text'].tolist()[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325f6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a summary\n",
    "generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d83a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the base t5 on the same text\n",
    "base_generator = pipeline(\n",
    "    'summarization', model='t5-small', tokenizer='t5-small'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02031ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary is a bit more extractive than our fine-tuned version and style isn't quite the same as our dataset\n",
    "base_generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84c0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "81f85b80f841f9e4ad476f9d6bcca26b36a8a21c8d79ecc7e545a85665969833"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
